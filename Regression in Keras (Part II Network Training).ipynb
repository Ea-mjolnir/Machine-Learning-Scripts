{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04 - Neural Networks for Regression in Keras\n",
    "\n",
    "## Part II - Constructing and training a model\n",
    "\n",
    "The main task of this exercise notebook is to build a neural network model of housing prices in California using the California census data. Our model should learn from this data and be able to predict the median housing price in any district, given all the other attributes.\n",
    "\n",
    "* This task is a typical **supervised learning task**, since we are given **labeled training examples** (each instance comes with the expected\n",
    "output, i.e., the district’s median housing price).\n",
    "* The task is also a typical **regression task**, since we are asked to predict a **continuous real value**.\n",
    "* More specifically, this is a **multiple regression problem** since the system will use **multiple features** to make a prediction (it\n",
    "will use the district’s population, the median income, etc.).\n",
    "* It is also a **univariate regression** problem since we are only trying to **predict a single value** for each district.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Learn how to configure a neural network architecture to solve a regression task, and in particular:\n",
    "- Learn to define the output layer for regression task\n",
    "- Learn to define activation functions for regression\n",
    "- Understand the cost functions and metrics for regression\n",
    "- Understand and practice weight and bias initialization in Keras\n",
    "- Try out Batch Normalization Layers\n",
    "- Learn how to take advantage of the logs dictionary in Keras\n",
    "- Learn to define our own callbacks and check the progress of a model\n",
    "- Understand and practice EarlyStoping \n",
    "\n",
    "Before you can start, find a GPU on the system that is not heavily used by other users (with **nvidia-smi**). Otherwise you cannot initialize your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change X to the GPU number you want to use,\n",
    "# otherwise you will get a Python error\n",
    "# e.g. USE_GPU = 4\n",
    "USE_GPU = 6 # YOUR CHOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "  /physical_device:GPU:6 GPU\n",
      "  /physical_device:GPU:7 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:6 GPU\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow \n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "USE_GPU = 6\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California housing dataset\n",
    "\n",
    "For more details on the dataset used in this exercise, please take a look at part I about exploratory data analysis.\n",
    "\n",
    "In the following, the data is loaded and stored in the respective folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : (20640, 8)\n",
      "Labels: (20640,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "if not os.path.exists('./datasets'):\n",
    "    os.mkdir('./datasets')\n",
    "    \n",
    "X_full, y_full = fetch_california_housing(data_home = r'./datasets', return_X_y=True)\n",
    "\n",
    "print(\"Input :\", X_full.shape)\n",
    "print(\"Labels:\", y_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and test sets\n",
    "\n",
    "Evaluating a model always boils down to splitting your available data into three sets:\n",
    "training, validation, and test set. You train on the training data, and evaluate your model on the validation data. Once your model is ready to use, you test it one final time\n",
    "on the test data The reason is that developing a model always involves tuning its hyperparameters, e.g. picking the number of layers or the size of the layers. You do this tuning based on the feeback you get from the validation data, so in essence this tuning is a form of learning: a search for a good configuration in some hyperparameter space. As a result, tuning\n",
    "the configuration of the model based on its performance on the validation set can quickly result in overfitting to the validation set, even though your model is never being directly\n",
    "trained on it.\n",
    "\n",
    "Central to this phenomenon is the notion of \"information leak\". Every time you are tuning a hyperparameter of your model based on the model’s performance on the validation set, some information about the validation data is leaking into your model. If you only do this once, for one parameter, then very few bits of information would be leaking and your validation set would remain a reliable way to evaluate your model. But if you repeat this many times, running one experiment, evaluating on the validation set, modifying your model as a result, then you are leaking an increasingly significant amount of information about the validation set into your model. At the end of the day, you end up with a model that performs artificially well on the validation data, because it is what you optimized it for. But what you want in the end is a model that performa also well on unseen data, which is the test set.\n",
    "\n",
    "Splitting your data into a training, validation, and test sets may seem straightforward, but there are a few advanced ways to do it which can come in handy when very few data is available. Scikit-Learn provides a several functions to split datasets into multiple subsets in various ways. The simplest function is [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train%20test%20split#sklearn.model_selection.train_test_split). It has a key-value argument `random_state parameter` that allows you to set the random generator seed, which allows us to reproduce the results. This is sometimes helpful when experimenting with your data. If you do not provide any value as seed value, the split would be totally random.\n",
    "\n",
    "We first split into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training and test dataset 16512,4128 samples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_full, y_full ,\n",
    "                                                 test_size = 0.2,random_state=42)\n",
    "\n",
    "print(f'The training and test dataset {x_train.shape[0]},{x_test.shape[0]} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And furthermore, we split the just received trainig data in an even smaller training set and a validation set. As usual, we will use the validation set to follow the progress of the training on samples not seen during the training which will help us to tune the training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training and validation data 12384,4128 samples\n"
     ]
    }
   ],
   "source": [
    "xS_train,x_valid,yS_train,y_valid =train_test_split(x_train,y_train,\n",
    "                                                       test_size =0.25,random_state=42)\n",
    "\n",
    "print(f'The training and validation data {xS_train.shape[0]},{x_valid.shape[0]} samples')\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the numerical input attributes have very different scales. This is the case for the housing data: the average number of rooms ranges from about 0.84 to 141, while the median income only ranges from 0 to 15. Note that scaling the *target values* is generally not required.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale: `min-max scaling` and `standardization`.\n",
    "- Normalization (min-max scaling) is a rescaling of the data from the original range so that all values are within the new range of 0.0 and 1.0.\n",
    "- Standardization requires that you know or are able to accurately estimate the mean and standard deviation of observable values. You may be able to estimate these values from your training data, not the entire dataset.\n",
    "Subtracting the mean from the data is called centering, whereas dividing by the standard deviation is called scaling. Therefore, the method is sometimes also called “center scaling“.\n",
    "Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0.0 to 1.0). However, standardization is much less affected by outliers.\n",
    "\n",
    "In order to standardise our data, we will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class of sklearn. After importing the `StandardScaler` class, we construct an object from it and assign it to a variable. Using the `fit_transform()` method of the scaler, we then fit the training data to the scaler, i.e. compute the mean and unit standard deviation for each input features, and transform the training data accordingly. The mean and unit standard deviation (or equivalently the unit variance) can now be accessed with the `mean_` and `var_` attributes of the scaler object. At this point, we can transform the validation and test data with the same transformation using the `transform()` method of the scaler.\n",
    "\n",
    "As with all the data transformations, it is important to **fit the scalers to the training data only!!!**, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (as well as unseen data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train means: [ 3.87008981e+00  2.85858366e+01  5.41183744e+00  1.09258608e+00\n",
      "  1.42728965e+03  3.03326420e+00  3.56420026e+01 -1.19579831e+02]\n",
      "\n",
      "Train standard deviations: [3.56515265e+00 1.58950642e+02 4.31559188e+00 1.30995274e-01\n",
      " 1.30172719e+06 5.00432321e+01 4.54069499e+00 4.02000907e+00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "X_train_scaled = std_scaler.fit_transform(xS_train)\n",
    "print(f'Train means: {std_scaler.mean_}\\n')\n",
    "print(f'Train standard deviations: {std_scaler.var_}\\n')\n",
    "X_valid_scaled = std_scaler.transform(x_valid)\n",
    "X_test_scaled  = std_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model for regression\n",
    "    \n",
    "Building, training, evaluating and using a regression neural network using the Sequential API to make predictions is quite similar to what we did for classification. Because the dataset is quite noisy and of reduced size, we will just use a single hidden layer with 32 neurons to avoid overfitting. The shape of the input layer is (8,), which is a vector of 8 values, our features from the housing data. In contrast to the classification task, where the output is a set of probabilities corresponding to each categories of a pre-defined set, the output of regression is a continuous, real number. Consequently, **the output layer has only a single neuron**, since we only want to predict a single value, and uses no activation function. (If we would predict n numbers, we would use n neurons in the output layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "model = Sequential([Dense(32, activation=\"relu\", input_shape=(8,)),Dense(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important difference (to classification) is the used loss function needed to train the network. Here, we use the mean squared error, referenced by the string `mean_squared_error` or `mse` in TensorFlow. (You can use either `mean_squared_error` or `mse`, it does not matter. The only difference is that the fit methods outputs the string in one way or the other, so in the first case, your output string is quite long.) The mean squared error takes the sum of the squared differences between the predicted values and the real values.\n",
    "\n",
    "Further popular loss functions are mean of absolute error (MAE) or Huber loss.\n",
    "\n",
    "When compiling our model, we define the loss function and the performance metrics to be the mean squared error. Actually, we would not even need the performance metrics, since the loss and the performance metrics are both the mse, and would give the same value. But it might be easier to read the output, since it now explicitly states the mse, and not just a loss value that we might have forgotten to be the mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), metrics=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can fit the model to the training data using the scaled training data, the (unscaled) labels of the training data, and evaluate by the scaled validation data, and the unscaled evaluation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 2.0104 - mse: 2.0104 - val_loss: 0.8721 - val_mse: 0.8721\n",
      "Epoch 2/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.7325 - mse: 0.7325 - val_loss: 0.6917 - val_mse: 0.6917\n",
      "Epoch 3/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.6493 - mse: 0.6493 - val_loss: 0.6399 - val_mse: 0.6399\n",
      "Epoch 4/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.6097 - mse: 0.6097 - val_loss: 0.6029 - val_mse: 0.6029\n",
      "Epoch 5/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5803 - mse: 0.5803 - val_loss: 0.5781 - val_mse: 0.5781\n",
      "Epoch 6/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5574 - mse: 0.5574 - val_loss: 0.5549 - val_mse: 0.5549\n",
      "Epoch 7/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5372 - mse: 0.5372 - val_loss: 0.5370 - val_mse: 0.5370\n",
      "Epoch 8/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5213 - mse: 0.5213 - val_loss: 0.5195 - val_mse: 0.5195\n",
      "Epoch 9/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5069 - mse: 0.5069 - val_loss: 0.5062 - val_mse: 0.5062\n",
      "Epoch 10/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4945 - mse: 0.4945 - val_loss: 0.4949 - val_mse: 0.4949\n",
      "Epoch 11/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4840 - mse: 0.4840 - val_loss: 0.4851 - val_mse: 0.4851\n",
      "Epoch 12/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4749 - mse: 0.4749 - val_loss: 0.4743 - val_mse: 0.4743\n",
      "Epoch 13/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4668 - mse: 0.4668 - val_loss: 0.4667 - val_mse: 0.4667\n",
      "Epoch 14/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4600 - mse: 0.4600 - val_loss: 0.4603 - val_mse: 0.4603\n",
      "Epoch 15/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4539 - mse: 0.4539 - val_loss: 0.4555 - val_mse: 0.4555\n",
      "Epoch 16/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4487 - mse: 0.4487 - val_loss: 0.4494 - val_mse: 0.4494\n",
      "Epoch 17/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4438 - mse: 0.4438 - val_loss: 0.4444 - val_mse: 0.4444\n",
      "Epoch 18/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.4417 - val_mse: 0.4417\n",
      "Epoch 19/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4360 - mse: 0.4360 - val_loss: 0.4360 - val_mse: 0.4360\n",
      "Epoch 20/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4330 - mse: 0.4330 - val_loss: 0.4330 - val_mse: 0.4330\n",
      "Epoch 21/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4298 - mse: 0.4298 - val_loss: 0.4326 - val_mse: 0.4326\n",
      "Epoch 22/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4271 - mse: 0.4271 - val_loss: 0.4270 - val_mse: 0.4270\n",
      "Epoch 23/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4247 - mse: 0.4247 - val_loss: 0.4245 - val_mse: 0.4245\n",
      "Epoch 24/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4223 - mse: 0.4223 - val_loss: 0.4231 - val_mse: 0.4231\n",
      "Epoch 25/25\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4201 - mse: 0.4201 - val_loss: 0.4225 - val_mse: 0.4225\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, yS_train,epochs=25, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping callback\n",
    "\n",
    "When training a model we cannot predict a-priori *how many epochs* will be needed to get to an *optimal validation loss*. One empirical method is to train a model for a fixed number of epochs, in general large enough that it would start overfitting, and using this first run to figure out the proper number of epochs to train, and finally launch a new training run from scratch with this optimal number of epochs.\n",
    "\n",
    "A much better way to handle this would be to stop training when we measure that the validation loss has stopped improving. This can be achieved using a Keras `callback` object. A callback is an object (a class instance implementing specific methods) that is passed to the `model` in the call to `fit` and that is called by the model at various points during training. It has access to all the data available about the state of the model and its performance, and it is capable to take action, for instance interrupting training, saving a model, loading a different weight set, or otherwise altering the state of the model (depending on the type of call back).\n",
    "\n",
    "**Callbacks can be used for:**\n",
    "- Model checkpointing: saving the current weights of the model at different points during training.\n",
    "- Early stopping: interrupting training when the validation loss has stopped improving (and of course, saving the best model obtained during training).\n",
    "- Dynamically adjusting the value of certain parameters during training, such as the learning rate of the optimizer.\n",
    "- Logging the training and validation metrics during training, or visualizing the representations learned by the model as they get updated. \n",
    "- And more …\n",
    "\n",
    "There are a number of built-in callbacks found in the `tf.keras.callbacks` [module](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks).\n",
    "\n",
    "In the following, we define an early stopping callback to interrupt the training process once the loss (MSE) has stopped improving for a defined number of epochs. In our example, the validation loss (`val_loss`) is monitored, and if the difference between two successive loss values is less than a specified value (`min_delta`) for a number of epochs (`patience`), then the training is stopped. Meaning, if the loss has not improved by this min_delta over the patience number of epochs, then this callback will stop the training process.\n",
    "\n",
    "Create a list of callbacks that contains, in our example, just an `EarlyStopping` class callback with the above described arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', min_delta = .001, patience = 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a new model, compile it, and to fit the data, we pass the list of callbacks to the fit() method of the model. This time we define the training process to use 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 2.3309 - mse: 2.3309 - val_loss: 1.0550 - val_mse: 1.0550\n",
      "Epoch 2/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.7575 - mse: 0.7575 - val_loss: 0.7221 - val_mse: 0.7221\n",
      "Epoch 3/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.6710 - mse: 0.6710 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 4/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.6347 - mse: 0.6347 - val_loss: 0.6415 - val_mse: 0.6415\n",
      "Epoch 5/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.6084 - mse: 0.6084 - val_loss: 0.6129 - val_mse: 0.6129\n",
      "Epoch 6/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5864 - mse: 0.5864 - val_loss: 0.5915 - val_mse: 0.5915\n",
      "Epoch 7/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5681 - mse: 0.5681 - val_loss: 0.5730 - val_mse: 0.5730\n",
      "Epoch 8/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5529 - mse: 0.5529 - val_loss: 0.5604 - val_mse: 0.5604\n",
      "Epoch 9/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5397 - mse: 0.5397 - val_loss: 0.5452 - val_mse: 0.5452\n",
      "Epoch 10/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5284 - mse: 0.5284 - val_loss: 0.5338 - val_mse: 0.5338\n",
      "Epoch 11/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5185 - mse: 0.5185 - val_loss: 0.5239 - val_mse: 0.5239\n",
      "Epoch 12/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5100 - mse: 0.5100 - val_loss: 0.5149 - val_mse: 0.5149\n",
      "Epoch 13/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.5028 - mse: 0.5028 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 14/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4965 - mse: 0.4965 - val_loss: 0.5040 - val_mse: 0.5040\n",
      "Epoch 15/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4912 - mse: 0.4912 - val_loss: 0.4993 - val_mse: 0.4993\n",
      "Epoch 16/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4865 - mse: 0.4865 - val_loss: 0.4918 - val_mse: 0.4918\n",
      "Epoch 17/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4877 - val_mse: 0.4877\n",
      "Epoch 18/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4784 - mse: 0.4784 - val_loss: 0.4827 - val_mse: 0.4827\n",
      "Epoch 19/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4752 - mse: 0.4752 - val_loss: 0.4790 - val_mse: 0.4790\n",
      "Epoch 20/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4717 - mse: 0.4717 - val_loss: 0.4762 - val_mse: 0.4762\n",
      "Epoch 21/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4688 - mse: 0.4688 - val_loss: 0.4766 - val_mse: 0.4766\n",
      "Epoch 22/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4656 - mse: 0.4656 - val_loss: 0.4688 - val_mse: 0.4688\n",
      "Epoch 23/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4632 - mse: 0.4632 - val_loss: 0.4680 - val_mse: 0.4680\n",
      "Epoch 24/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4605 - mse: 0.4605 - val_loss: 0.4648 - val_mse: 0.4648\n",
      "Epoch 25/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4575 - mse: 0.4575 - val_loss: 0.4608 - val_mse: 0.4608\n",
      "Epoch 26/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4555 - mse: 0.4555 - val_loss: 0.4599 - val_mse: 0.4599\n",
      "Epoch 27/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4530 - mse: 0.4530 - val_loss: 0.4553 - val_mse: 0.4553\n",
      "Epoch 28/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4508 - mse: 0.4508 - val_loss: 0.4549 - val_mse: 0.4549\n",
      "Epoch 29/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4490 - mse: 0.4490 - val_loss: 0.4513 - val_mse: 0.4513\n",
      "Epoch 30/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4462 - mse: 0.4462 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 31/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4443 - mse: 0.4443 - val_loss: 0.4492 - val_mse: 0.4492\n",
      "Epoch 32/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4422 - mse: 0.4422 - val_loss: 0.4453 - val_mse: 0.4453\n",
      "Epoch 33/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4401 - mse: 0.4401 - val_loss: 0.4422 - val_mse: 0.4422\n",
      "Epoch 34/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4376 - mse: 0.4376 - val_loss: 0.4387 - val_mse: 0.4387\n",
      "Epoch 35/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4356 - mse: 0.4356 - val_loss: 0.4366 - val_mse: 0.4366\n",
      "Epoch 36/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4335 - mse: 0.4335 - val_loss: 0.4349 - val_mse: 0.4349\n",
      "Epoch 37/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4312 - mse: 0.4312 - val_loss: 0.4324 - val_mse: 0.4324\n",
      "Epoch 38/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4289 - mse: 0.4289 - val_loss: 0.4289 - val_mse: 0.4289\n",
      "Epoch 39/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4269 - mse: 0.4269 - val_loss: 0.4266 - val_mse: 0.4266\n",
      "Epoch 40/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4253 - mse: 0.4253 - val_loss: 0.4240 - val_mse: 0.4240\n",
      "Epoch 41/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4237 - mse: 0.4237 - val_loss: 0.4233 - val_mse: 0.4233\n",
      "Epoch 42/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4221 - mse: 0.4221 - val_loss: 0.4215 - val_mse: 0.4215\n",
      "Epoch 43/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4206 - mse: 0.4206 - val_loss: 0.4195 - val_mse: 0.4195\n",
      "Epoch 44/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4189 - mse: 0.4189 - val_loss: 0.4199 - val_mse: 0.4199\n",
      "Epoch 45/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4176 - mse: 0.4176 - val_loss: 0.4170 - val_mse: 0.4170\n",
      "Epoch 46/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4164 - mse: 0.4164 - val_loss: 0.4157 - val_mse: 0.4157\n",
      "Epoch 47/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4150 - mse: 0.4150 - val_loss: 0.4148 - val_mse: 0.4148\n",
      "Epoch 48/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4136 - mse: 0.4136 - val_loss: 0.4129 - val_mse: 0.4129\n",
      "Epoch 49/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4123 - mse: 0.4123 - val_loss: 0.4118 - val_mse: 0.4118\n",
      "Epoch 50/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4109 - mse: 0.4109 - val_loss: 0.4136 - val_mse: 0.4136\n",
      "Epoch 51/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4099 - mse: 0.4099 - val_loss: 0.4093 - val_mse: 0.4093\n",
      "Epoch 52/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4086 - mse: 0.4086 - val_loss: 0.4080 - val_mse: 0.4080\n",
      "Epoch 53/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4075 - mse: 0.4075 - val_loss: 0.4078 - val_mse: 0.4078\n",
      "Epoch 54/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4063 - mse: 0.4063 - val_loss: 0.4062 - val_mse: 0.4062\n",
      "Epoch 55/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4051 - mse: 0.4051 - val_loss: 0.4062 - val_mse: 0.4062\n",
      "Epoch 56/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 0.4046 - val_mse: 0.4046\n",
      "Epoch 57/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4031 - mse: 0.4031 - val_loss: 0.4035 - val_mse: 0.4035\n",
      "Epoch 58/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4020 - mse: 0.4020 - val_loss: 0.4029 - val_mse: 0.4029\n",
      "Epoch 59/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4009 - mse: 0.4009 - val_loss: 0.4017 - val_mse: 0.4017\n",
      "Epoch 60/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.4001 - mse: 0.4001 - val_loss: 0.4016 - val_mse: 0.4016\n",
      "Epoch 61/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3990 - mse: 0.3990 - val_loss: 0.4004 - val_mse: 0.4004\n",
      "Epoch 62/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3983 - mse: 0.3983 - val_loss: 0.3992 - val_mse: 0.3992\n",
      "Epoch 63/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3975 - mse: 0.3975 - val_loss: 0.3982 - val_mse: 0.3982\n",
      "Epoch 64/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3966 - mse: 0.3966 - val_loss: 0.3982 - val_mse: 0.3982\n",
      "Epoch 65/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3956 - mse: 0.3956 - val_loss: 0.3973 - val_mse: 0.3973\n",
      "Epoch 66/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3948 - mse: 0.3948 - val_loss: 0.3990 - val_mse: 0.3990\n",
      "Epoch 67/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3942 - mse: 0.3942 - val_loss: 0.3961 - val_mse: 0.3961\n",
      "Epoch 68/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3932 - mse: 0.3932 - val_loss: 0.3952 - val_mse: 0.3952\n",
      "Epoch 69/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3925 - mse: 0.3925 - val_loss: 0.3953 - val_mse: 0.3953\n",
      "Epoch 70/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3918 - mse: 0.3918 - val_loss: 0.3942 - val_mse: 0.3942\n",
      "Epoch 71/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3911 - mse: 0.3911 - val_loss: 0.3925 - val_mse: 0.3925\n",
      "Epoch 72/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3900 - mse: 0.3900 - val_loss: 0.3934 - val_mse: 0.3934\n",
      "Epoch 73/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3897 - mse: 0.3897 - val_loss: 0.3916 - val_mse: 0.3916\n",
      "Epoch 74/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3889 - mse: 0.3889 - val_loss: 0.3908 - val_mse: 0.3908\n",
      "Epoch 75/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3880 - mse: 0.3880 - val_loss: 0.3896 - val_mse: 0.3896\n",
      "Epoch 76/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3876 - mse: 0.3876 - val_loss: 0.3889 - val_mse: 0.3889\n",
      "Epoch 77/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3868 - mse: 0.3868 - val_loss: 0.3884 - val_mse: 0.3884\n",
      "Epoch 78/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3859 - mse: 0.3859 - val_loss: 0.3887 - val_mse: 0.3887\n",
      "Epoch 79/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3852 - mse: 0.3852 - val_loss: 0.3879 - val_mse: 0.3879\n",
      "Epoch 80/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3848 - mse: 0.3848 - val_loss: 0.3878 - val_mse: 0.3878\n",
      "Epoch 81/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3840 - mse: 0.3840 - val_loss: 0.3868 - val_mse: 0.3868\n",
      "Epoch 82/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3834 - mse: 0.3834 - val_loss: 0.3853 - val_mse: 0.3853\n",
      "Epoch 83/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3828 - mse: 0.3828 - val_loss: 0.3848 - val_mse: 0.3848\n",
      "Epoch 84/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3822 - mse: 0.3822 - val_loss: 0.3844 - val_mse: 0.3844\n",
      "Epoch 85/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3815 - mse: 0.3815 - val_loss: 0.3850 - val_mse: 0.3850\n",
      "Epoch 86/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3808 - mse: 0.3808 - val_loss: 0.3836 - val_mse: 0.3836\n",
      "Epoch 87/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3802 - mse: 0.3802 - val_loss: 0.3837 - val_mse: 0.3837\n",
      "Epoch 88/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3795 - mse: 0.3795 - val_loss: 0.3834 - val_mse: 0.3834\n",
      "Epoch 89/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3788 - mse: 0.3788 - val_loss: 0.3818 - val_mse: 0.3818\n",
      "Epoch 90/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3785 - mse: 0.3785 - val_loss: 0.3810 - val_mse: 0.3810\n",
      "Epoch 91/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3779 - mse: 0.3779 - val_loss: 0.3807 - val_mse: 0.3807\n",
      "Epoch 92/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3771 - mse: 0.3771 - val_loss: 0.3801 - val_mse: 0.3801\n",
      "Epoch 93/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3767 - mse: 0.3767 - val_loss: 0.3803 - val_mse: 0.3803\n",
      "Epoch 94/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3761 - mse: 0.3761 - val_loss: 0.3798 - val_mse: 0.3798\n",
      "Epoch 95/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3756 - mse: 0.3756 - val_loss: 0.3787 - val_mse: 0.3787\n",
      "Epoch 96/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3751 - mse: 0.3751 - val_loss: 0.3781 - val_mse: 0.3781\n",
      "Epoch 97/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3745 - mse: 0.3745 - val_loss: 0.3781 - val_mse: 0.3781\n",
      "Epoch 98/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3740 - mse: 0.3740 - val_loss: 0.3782 - val_mse: 0.3782\n",
      "Epoch 99/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3733 - mse: 0.3733 - val_loss: 0.3772 - val_mse: 0.3772\n",
      "Epoch 100/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.3728 - mse: 0.3728 - val_loss: 0.3774 - val_mse: 0.3774\n"
     ]
    }
   ],
   "source": [
    "# it does not converge due to the fact that the min_delta value is too small for the \n",
    "# number of epoch given 100 to converge.\n",
    "# I do not know if the optimizer used is not ideal for this model.\n",
    "\n",
    "callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', min_delta = .001, patience = 10)]\n",
    "model = Sequential([Dense(32, activation=\"relu\", input_shape=(8,)),Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), metrics=\"mse\")\n",
    "history = model.fit(X_train_scaled, yS_train,epochs=100,validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After around 64 epochs, the training process stops, because the loss value of the validation data did not improve anymore in the last 8 epochs by the specified value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training history\n",
    "The call to `model.fit()` returns a `History` object. This object has a member `history`, which is a dictionary containing the values of the loss and chosen metrics on the training and validation set during each training epoch. Since the 'history' attribute is a python dictionary, you can retrive the keys using the `.keys()` method. The `logs` dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n",
    "\n",
    "In the following, a method `plot_loss()` is implemented that takes this history object and plots the training and validation loss using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plt.plot(loss, label='loss')\n",
    "    plt.plot(val_loss, label='val_loss')\n",
    "    plt.ylim([0.2, 1.6])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss ')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then called with the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that although the training loss (blue line) decreases until the end of the training process, the validation loss plateaus much earlier. The model probably starts to overfit at around epoch 25 or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test data\n",
    "\n",
    "Using the `evaluate()` method of the model on the test data, we can get the evaluation metrics, here MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 773us/step - loss: 0.3870 - mse: 0.3870\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error on the test data seems to be the same as for the validation data. So we can conclude, that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Finally, let us make some predictions on the test data using the `predict()` method. And display the first 10 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6888725],\n",
       "       [1.7078247],\n",
       "       [3.5952587],\n",
       "       [2.7903302],\n",
       "       [2.6968858]], dtype=float32)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also output the real target values to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.477  , 0.458  , 5.00001, 2.186  , 2.78   ])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be reminded, that the predicted housing prices are in \\\\$100.000. Although the predicted results seem to be quite off, the model still follows the general trend of the prices. High prices are high, and low prices are low.\n",
    "\n",
    "The next function takes the true target values and the predicted house prices and plots a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff(y_true, y_pred, title = 'Linear_Regression'):\n",
    "    plt.scatter(y_true, y_pred)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.axis = ('equal')\n",
    "    plt.axis = ('square')\n",
    "    plt.xlim(plt.xlim())\n",
    "    plt.ylim(plt.ylim())\n",
    "    plt.plot([-500, 500], [-500, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_diff(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model follows the general trend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
